{
  "best_metric": 0.7724015046525441,
  "best_model_checkpoint": "d:\\Sarcasm Detection\\outputs\\baseline_full\\checkpoint-227424",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 227424,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004397073308006191,
      "grad_norm": 16.10238265991211,
      "learning_rate": 1.9970686177946626e-05,
      "loss": 0.6578,
      "step": 500
    },
    {
      "epoch": 0.008794146616012383,
      "grad_norm": 21.14532470703125,
      "learning_rate": 1.994137235589325e-05,
      "loss": 0.608,
      "step": 1000
    },
    {
      "epoch": 0.013191219924018574,
      "grad_norm": 12.89415454864502,
      "learning_rate": 1.9912058533839876e-05,
      "loss": 0.5992,
      "step": 1500
    },
    {
      "epoch": 0.017588293232024765,
      "grad_norm": 8.76850414276123,
      "learning_rate": 1.9882744711786504e-05,
      "loss": 0.5907,
      "step": 2000
    },
    {
      "epoch": 0.021985366540030957,
      "grad_norm": 14.79373836517334,
      "learning_rate": 1.985343088973313e-05,
      "loss": 0.5853,
      "step": 2500
    },
    {
      "epoch": 0.026382439848037148,
      "grad_norm": 11.913227081298828,
      "learning_rate": 1.9824117067679754e-05,
      "loss": 0.5757,
      "step": 3000
    },
    {
      "epoch": 0.030779513156043336,
      "grad_norm": 11.260076522827148,
      "learning_rate": 1.979480324562638e-05,
      "loss": 0.5749,
      "step": 3500
    },
    {
      "epoch": 0.03517658646404953,
      "grad_norm": 25.15863800048828,
      "learning_rate": 1.9765489423573007e-05,
      "loss": 0.5669,
      "step": 4000
    },
    {
      "epoch": 0.03957365977205572,
      "grad_norm": 15.691210746765137,
      "learning_rate": 1.973617560151963e-05,
      "loss": 0.5727,
      "step": 4500
    },
    {
      "epoch": 0.04397073308006191,
      "grad_norm": 8.571742057800293,
      "learning_rate": 1.9706861779466257e-05,
      "loss": 0.5757,
      "step": 5000
    },
    {
      "epoch": 0.048367806388068105,
      "grad_norm": 5.872710227966309,
      "learning_rate": 1.967754795741288e-05,
      "loss": 0.5628,
      "step": 5500
    },
    {
      "epoch": 0.052764879696074296,
      "grad_norm": 17.474029541015625,
      "learning_rate": 1.9648234135359506e-05,
      "loss": 0.5532,
      "step": 6000
    },
    {
      "epoch": 0.05716195300408049,
      "grad_norm": 8.46614933013916,
      "learning_rate": 1.961892031330613e-05,
      "loss": 0.5517,
      "step": 6500
    },
    {
      "epoch": 0.06155902631208667,
      "grad_norm": 10.339386940002441,
      "learning_rate": 1.9589606491252756e-05,
      "loss": 0.5549,
      "step": 7000
    },
    {
      "epoch": 0.06595609962009287,
      "grad_norm": 9.426886558532715,
      "learning_rate": 1.9560292669199384e-05,
      "loss": 0.5553,
      "step": 7500
    },
    {
      "epoch": 0.07035317292809906,
      "grad_norm": 14.531909942626953,
      "learning_rate": 1.9530978847146005e-05,
      "loss": 0.5612,
      "step": 8000
    },
    {
      "epoch": 0.07475024623610525,
      "grad_norm": 13.376687049865723,
      "learning_rate": 1.9501665025092634e-05,
      "loss": 0.55,
      "step": 8500
    },
    {
      "epoch": 0.07914731954411144,
      "grad_norm": 10.520232200622559,
      "learning_rate": 1.947235120303926e-05,
      "loss": 0.5471,
      "step": 9000
    },
    {
      "epoch": 0.08354439285211764,
      "grad_norm": 9.135151863098145,
      "learning_rate": 1.9443037380985883e-05,
      "loss": 0.5517,
      "step": 9500
    },
    {
      "epoch": 0.08794146616012383,
      "grad_norm": 10.544546127319336,
      "learning_rate": 1.9413723558932508e-05,
      "loss": 0.5645,
      "step": 10000
    },
    {
      "epoch": 0.09233853946813002,
      "grad_norm": 6.292418479919434,
      "learning_rate": 1.9384409736879136e-05,
      "loss": 0.5542,
      "step": 10500
    },
    {
      "epoch": 0.09673561277613621,
      "grad_norm": 4.757957458496094,
      "learning_rate": 1.935509591482576e-05,
      "loss": 0.5508,
      "step": 11000
    },
    {
      "epoch": 0.1011326860841424,
      "grad_norm": 10.198322296142578,
      "learning_rate": 1.9325782092772386e-05,
      "loss": 0.5372,
      "step": 11500
    },
    {
      "epoch": 0.10552975939214859,
      "grad_norm": 18.386137008666992,
      "learning_rate": 1.929646827071901e-05,
      "loss": 0.5437,
      "step": 12000
    },
    {
      "epoch": 0.10992683270015478,
      "grad_norm": 5.64822244644165,
      "learning_rate": 1.9267154448665636e-05,
      "loss": 0.5547,
      "step": 12500
    },
    {
      "epoch": 0.11432390600816097,
      "grad_norm": 13.826244354248047,
      "learning_rate": 1.923784062661226e-05,
      "loss": 0.5497,
      "step": 13000
    },
    {
      "epoch": 0.11872097931616715,
      "grad_norm": 3.0177643299102783,
      "learning_rate": 1.9208526804558885e-05,
      "loss": 0.5374,
      "step": 13500
    },
    {
      "epoch": 0.12311805262417334,
      "grad_norm": 11.514457702636719,
      "learning_rate": 1.9179212982505513e-05,
      "loss": 0.5431,
      "step": 14000
    },
    {
      "epoch": 0.12751512593217953,
      "grad_norm": 4.08184814453125,
      "learning_rate": 1.9149899160452138e-05,
      "loss": 0.5493,
      "step": 14500
    },
    {
      "epoch": 0.13191219924018574,
      "grad_norm": 16.227306365966797,
      "learning_rate": 1.9120585338398763e-05,
      "loss": 0.5474,
      "step": 15000
    },
    {
      "epoch": 0.13630927254819192,
      "grad_norm": 6.341619491577148,
      "learning_rate": 1.9091271516345388e-05,
      "loss": 0.5506,
      "step": 15500
    },
    {
      "epoch": 0.14070634585619812,
      "grad_norm": 3.819018602371216,
      "learning_rate": 1.9061957694292016e-05,
      "loss": 0.5511,
      "step": 16000
    },
    {
      "epoch": 0.1451034191642043,
      "grad_norm": 8.470325469970703,
      "learning_rate": 1.9032643872238638e-05,
      "loss": 0.5624,
      "step": 16500
    },
    {
      "epoch": 0.1495004924722105,
      "grad_norm": 14.623211860656738,
      "learning_rate": 1.9003330050185266e-05,
      "loss": 0.5509,
      "step": 17000
    },
    {
      "epoch": 0.15389756578021668,
      "grad_norm": 14.522096633911133,
      "learning_rate": 1.897401622813189e-05,
      "loss": 0.5403,
      "step": 17500
    },
    {
      "epoch": 0.1582946390882229,
      "grad_norm": 8.34521198272705,
      "learning_rate": 1.8944702406078515e-05,
      "loss": 0.5474,
      "step": 18000
    },
    {
      "epoch": 0.16269171239622907,
      "grad_norm": 11.786779403686523,
      "learning_rate": 1.891538858402514e-05,
      "loss": 0.5326,
      "step": 18500
    },
    {
      "epoch": 0.16708878570423527,
      "grad_norm": 5.619934558868408,
      "learning_rate": 1.8886074761971765e-05,
      "loss": 0.5348,
      "step": 19000
    },
    {
      "epoch": 0.17148585901224145,
      "grad_norm": 5.259317398071289,
      "learning_rate": 1.8856760939918393e-05,
      "loss": 0.5441,
      "step": 19500
    },
    {
      "epoch": 0.17588293232024765,
      "grad_norm": 7.808330059051514,
      "learning_rate": 1.8827447117865018e-05,
      "loss": 0.5394,
      "step": 20000
    },
    {
      "epoch": 0.18028000562825383,
      "grad_norm": 1.828729510307312,
      "learning_rate": 1.8798133295811643e-05,
      "loss": 0.5472,
      "step": 20500
    },
    {
      "epoch": 0.18467707893626004,
      "grad_norm": 7.6771016120910645,
      "learning_rate": 1.8768819473758268e-05,
      "loss": 0.5536,
      "step": 21000
    },
    {
      "epoch": 0.1890741522442662,
      "grad_norm": 13.306890487670898,
      "learning_rate": 1.8739505651704893e-05,
      "loss": 0.5458,
      "step": 21500
    },
    {
      "epoch": 0.19347122555227242,
      "grad_norm": 14.329146385192871,
      "learning_rate": 1.8710191829651517e-05,
      "loss": 0.5538,
      "step": 22000
    },
    {
      "epoch": 0.1978682988602786,
      "grad_norm": 4.008246898651123,
      "learning_rate": 1.8680878007598146e-05,
      "loss": 0.5333,
      "step": 22500
    },
    {
      "epoch": 0.2022653721682848,
      "grad_norm": 17.859228134155273,
      "learning_rate": 1.865156418554477e-05,
      "loss": 0.5333,
      "step": 23000
    },
    {
      "epoch": 0.20666244547629098,
      "grad_norm": 5.842654705047607,
      "learning_rate": 1.8622250363491395e-05,
      "loss": 0.5296,
      "step": 23500
    },
    {
      "epoch": 0.21105951878429718,
      "grad_norm": 14.41653823852539,
      "learning_rate": 1.859293654143802e-05,
      "loss": 0.5511,
      "step": 24000
    },
    {
      "epoch": 0.21545659209230336,
      "grad_norm": 4.503015518188477,
      "learning_rate": 1.8563622719384645e-05,
      "loss": 0.5454,
      "step": 24500
    },
    {
      "epoch": 0.21985366540030957,
      "grad_norm": 4.911384582519531,
      "learning_rate": 1.853430889733127e-05,
      "loss": 0.5495,
      "step": 25000
    },
    {
      "epoch": 0.22425073870831574,
      "grad_norm": 7.513529300689697,
      "learning_rate": 1.8504995075277898e-05,
      "loss": 0.5389,
      "step": 25500
    },
    {
      "epoch": 0.22864781201632195,
      "grad_norm": 6.642040729522705,
      "learning_rate": 1.8475681253224523e-05,
      "loss": 0.5338,
      "step": 26000
    },
    {
      "epoch": 0.23304488532432813,
      "grad_norm": 4.212671756744385,
      "learning_rate": 1.8446367431171147e-05,
      "loss": 0.5428,
      "step": 26500
    },
    {
      "epoch": 0.2374419586323343,
      "grad_norm": 42.86201095581055,
      "learning_rate": 1.8417053609117772e-05,
      "loss": 0.5378,
      "step": 27000
    },
    {
      "epoch": 0.2418390319403405,
      "grad_norm": 5.76241397857666,
      "learning_rate": 1.8387739787064397e-05,
      "loss": 0.5414,
      "step": 27500
    },
    {
      "epoch": 0.2462361052483467,
      "grad_norm": 7.597986221313477,
      "learning_rate": 1.8358425965011025e-05,
      "loss": 0.5303,
      "step": 28000
    },
    {
      "epoch": 0.25063317855635286,
      "grad_norm": 4.6899495124816895,
      "learning_rate": 1.8329112142957647e-05,
      "loss": 0.5348,
      "step": 28500
    },
    {
      "epoch": 0.25503025186435907,
      "grad_norm": 5.690488815307617,
      "learning_rate": 1.8299798320904275e-05,
      "loss": 0.5382,
      "step": 29000
    },
    {
      "epoch": 0.2594273251723653,
      "grad_norm": 5.866443157196045,
      "learning_rate": 1.82704844988509e-05,
      "loss": 0.5456,
      "step": 29500
    },
    {
      "epoch": 0.2638243984803715,
      "grad_norm": 12.43000316619873,
      "learning_rate": 1.8241170676797525e-05,
      "loss": 0.5572,
      "step": 30000
    },
    {
      "epoch": 0.26822147178837763,
      "grad_norm": 12.604355812072754,
      "learning_rate": 1.821185685474415e-05,
      "loss": 0.5521,
      "step": 30500
    },
    {
      "epoch": 0.27261854509638384,
      "grad_norm": 2.2164981365203857,
      "learning_rate": 1.8182543032690774e-05,
      "loss": 0.5444,
      "step": 31000
    },
    {
      "epoch": 0.27701561840439004,
      "grad_norm": 5.7003350257873535,
      "learning_rate": 1.8153229210637402e-05,
      "loss": 0.5512,
      "step": 31500
    },
    {
      "epoch": 0.28141269171239625,
      "grad_norm": 36.822967529296875,
      "learning_rate": 1.8123915388584027e-05,
      "loss": 0.5331,
      "step": 32000
    },
    {
      "epoch": 0.2858097650204024,
      "grad_norm": 15.342398643493652,
      "learning_rate": 1.8094601566530652e-05,
      "loss": 0.559,
      "step": 32500
    },
    {
      "epoch": 0.2902068383284086,
      "grad_norm": 13.210451126098633,
      "learning_rate": 1.8065287744477277e-05,
      "loss": 0.5339,
      "step": 33000
    },
    {
      "epoch": 0.2946039116364148,
      "grad_norm": 4.111410617828369,
      "learning_rate": 1.8035973922423905e-05,
      "loss": 0.5482,
      "step": 33500
    },
    {
      "epoch": 0.299000984944421,
      "grad_norm": 5.225286960601807,
      "learning_rate": 1.8006660100370527e-05,
      "loss": 0.5618,
      "step": 34000
    },
    {
      "epoch": 0.30339805825242716,
      "grad_norm": 4.838624954223633,
      "learning_rate": 1.7977346278317155e-05,
      "loss": 0.5502,
      "step": 34500
    },
    {
      "epoch": 0.30779513156043337,
      "grad_norm": 3.6266520023345947,
      "learning_rate": 1.794803245626378e-05,
      "loss": 0.5467,
      "step": 35000
    },
    {
      "epoch": 0.31219220486843957,
      "grad_norm": 28.52631187438965,
      "learning_rate": 1.7918718634210404e-05,
      "loss": 0.5657,
      "step": 35500
    },
    {
      "epoch": 0.3165892781764458,
      "grad_norm": 5.3708906173706055,
      "learning_rate": 1.788940481215703e-05,
      "loss": 0.5393,
      "step": 36000
    },
    {
      "epoch": 0.3209863514844519,
      "grad_norm": 12.585262298583984,
      "learning_rate": 1.7860090990103654e-05,
      "loss": 0.5381,
      "step": 36500
    },
    {
      "epoch": 0.32538342479245813,
      "grad_norm": 4.328180313110352,
      "learning_rate": 1.783077716805028e-05,
      "loss": 0.5498,
      "step": 37000
    },
    {
      "epoch": 0.32978049810046434,
      "grad_norm": 20.08994483947754,
      "learning_rate": 1.7801463345996907e-05,
      "loss": 0.5473,
      "step": 37500
    },
    {
      "epoch": 0.33417757140847054,
      "grad_norm": 10.494210243225098,
      "learning_rate": 1.7772149523943532e-05,
      "loss": 0.5394,
      "step": 38000
    },
    {
      "epoch": 0.3385746447164767,
      "grad_norm": 11.335219383239746,
      "learning_rate": 1.7742835701890157e-05,
      "loss": 0.5422,
      "step": 38500
    },
    {
      "epoch": 0.3429717180244829,
      "grad_norm": 7.310627460479736,
      "learning_rate": 1.771352187983678e-05,
      "loss": 0.5612,
      "step": 39000
    },
    {
      "epoch": 0.3473687913324891,
      "grad_norm": 1.660744547843933,
      "learning_rate": 1.7684208057783406e-05,
      "loss": 0.5931,
      "step": 39500
    },
    {
      "epoch": 0.3517658646404953,
      "grad_norm": 13.63072395324707,
      "learning_rate": 1.7654894235730035e-05,
      "loss": 0.5906,
      "step": 40000
    },
    {
      "epoch": 0.35616293794850146,
      "grad_norm": 6.712850093841553,
      "learning_rate": 1.7625580413676656e-05,
      "loss": 0.5573,
      "step": 40500
    },
    {
      "epoch": 0.36056001125650766,
      "grad_norm": 15.438429832458496,
      "learning_rate": 1.7596266591623284e-05,
      "loss": 0.5758,
      "step": 41000
    },
    {
      "epoch": 0.36495708456451387,
      "grad_norm": 4.093373775482178,
      "learning_rate": 1.756695276956991e-05,
      "loss": 0.5623,
      "step": 41500
    },
    {
      "epoch": 0.3693541578725201,
      "grad_norm": 5.594182014465332,
      "learning_rate": 1.7537638947516534e-05,
      "loss": 0.5534,
      "step": 42000
    },
    {
      "epoch": 0.3737512311805262,
      "grad_norm": 70.36799621582031,
      "learning_rate": 1.750832512546316e-05,
      "loss": 0.559,
      "step": 42500
    },
    {
      "epoch": 0.3781483044885324,
      "grad_norm": 12.74493408203125,
      "learning_rate": 1.7479011303409787e-05,
      "loss": 0.5336,
      "step": 43000
    },
    {
      "epoch": 0.38254537779653863,
      "grad_norm": 6.730343341827393,
      "learning_rate": 1.744969748135641e-05,
      "loss": 0.5485,
      "step": 43500
    },
    {
      "epoch": 0.38694245110454484,
      "grad_norm": 7.61268949508667,
      "learning_rate": 1.7420383659303036e-05,
      "loss": 0.5569,
      "step": 44000
    },
    {
      "epoch": 0.391339524412551,
      "grad_norm": 22.03912925720215,
      "learning_rate": 1.739106983724966e-05,
      "loss": 0.5586,
      "step": 44500
    },
    {
      "epoch": 0.3957365977205572,
      "grad_norm": 5.981062412261963,
      "learning_rate": 1.7361756015196286e-05,
      "loss": 0.541,
      "step": 45000
    },
    {
      "epoch": 0.4001336710285634,
      "grad_norm": 5.022045135498047,
      "learning_rate": 1.7332442193142914e-05,
      "loss": 0.5719,
      "step": 45500
    },
    {
      "epoch": 0.4045307443365696,
      "grad_norm": 52.3754768371582,
      "learning_rate": 1.7303128371089536e-05,
      "loss": 0.558,
      "step": 46000
    },
    {
      "epoch": 0.40892781764457575,
      "grad_norm": 4.643350124359131,
      "learning_rate": 1.7273814549036164e-05,
      "loss": 0.5599,
      "step": 46500
    },
    {
      "epoch": 0.41332489095258196,
      "grad_norm": 16.152191162109375,
      "learning_rate": 1.724450072698279e-05,
      "loss": 0.5463,
      "step": 47000
    },
    {
      "epoch": 0.41772196426058816,
      "grad_norm": 4.530374050140381,
      "learning_rate": 1.7215186904929414e-05,
      "loss": 0.531,
      "step": 47500
    },
    {
      "epoch": 0.42211903756859437,
      "grad_norm": 3.6815121173858643,
      "learning_rate": 1.718587308287604e-05,
      "loss": 0.5354,
      "step": 48000
    },
    {
      "epoch": 0.4265161108766005,
      "grad_norm": 21.64809226989746,
      "learning_rate": 1.7156559260822667e-05,
      "loss": 0.5354,
      "step": 48500
    },
    {
      "epoch": 0.4309131841846067,
      "grad_norm": 4.70950984954834,
      "learning_rate": 1.7127245438769288e-05,
      "loss": 0.5443,
      "step": 49000
    },
    {
      "epoch": 0.43531025749261293,
      "grad_norm": 12.579290390014648,
      "learning_rate": 1.7097931616715916e-05,
      "loss": 0.5373,
      "step": 49500
    },
    {
      "epoch": 0.43970733080061913,
      "grad_norm": 5.780561923980713,
      "learning_rate": 1.706861779466254e-05,
      "loss": 0.5376,
      "step": 50000
    },
    {
      "epoch": 0.4441044041086253,
      "grad_norm": 2.402400493621826,
      "learning_rate": 1.7039303972609166e-05,
      "loss": 0.5447,
      "step": 50500
    },
    {
      "epoch": 0.4485014774166315,
      "grad_norm": 4.992504596710205,
      "learning_rate": 1.700999015055579e-05,
      "loss": 0.5457,
      "step": 51000
    },
    {
      "epoch": 0.4528985507246377,
      "grad_norm": 3.238271713256836,
      "learning_rate": 1.6980676328502415e-05,
      "loss": 0.5398,
      "step": 51500
    },
    {
      "epoch": 0.4572956240326439,
      "grad_norm": 9.454764366149902,
      "learning_rate": 1.6951362506449044e-05,
      "loss": 0.5563,
      "step": 52000
    },
    {
      "epoch": 0.46169269734065005,
      "grad_norm": 2.385955810546875,
      "learning_rate": 1.6922048684395665e-05,
      "loss": 0.5567,
      "step": 52500
    },
    {
      "epoch": 0.46608977064865625,
      "grad_norm": 11.145878791809082,
      "learning_rate": 1.6892734862342293e-05,
      "loss": 0.5259,
      "step": 53000
    },
    {
      "epoch": 0.47048684395666246,
      "grad_norm": 6.780707836151123,
      "learning_rate": 1.6863421040288918e-05,
      "loss": 0.5434,
      "step": 53500
    },
    {
      "epoch": 0.4748839172646686,
      "grad_norm": 5.413635730743408,
      "learning_rate": 1.6834107218235546e-05,
      "loss": 0.5291,
      "step": 54000
    },
    {
      "epoch": 0.4792809905726748,
      "grad_norm": 4.460791110992432,
      "learning_rate": 1.6804793396182168e-05,
      "loss": 0.5459,
      "step": 54500
    },
    {
      "epoch": 0.483678063880681,
      "grad_norm": 8.466111183166504,
      "learning_rate": 1.6775479574128796e-05,
      "loss": 0.5294,
      "step": 55000
    },
    {
      "epoch": 0.4880751371886872,
      "grad_norm": 6.748472690582275,
      "learning_rate": 1.674616575207542e-05,
      "loss": 0.529,
      "step": 55500
    },
    {
      "epoch": 0.4924722104966934,
      "grad_norm": 37.749053955078125,
      "learning_rate": 1.6716851930022046e-05,
      "loss": 0.5314,
      "step": 56000
    },
    {
      "epoch": 0.4968692838046996,
      "grad_norm": 5.334331035614014,
      "learning_rate": 1.668753810796867e-05,
      "loss": 0.5372,
      "step": 56500
    },
    {
      "epoch": 0.5012663571127057,
      "grad_norm": 9.814000129699707,
      "learning_rate": 1.6658224285915295e-05,
      "loss": 0.5337,
      "step": 57000
    },
    {
      "epoch": 0.5056634304207119,
      "grad_norm": 7.834160804748535,
      "learning_rate": 1.6628910463861923e-05,
      "loss": 0.5523,
      "step": 57500
    },
    {
      "epoch": 0.5100605037287181,
      "grad_norm": 1.569204568862915,
      "learning_rate": 1.6599596641808545e-05,
      "loss": 0.5577,
      "step": 58000
    },
    {
      "epoch": 0.5144575770367243,
      "grad_norm": 1.9723289012908936,
      "learning_rate": 1.6570282819755173e-05,
      "loss": 0.564,
      "step": 58500
    },
    {
      "epoch": 0.5188546503447306,
      "grad_norm": 27.5350399017334,
      "learning_rate": 1.6540968997701798e-05,
      "loss": 0.5722,
      "step": 59000
    },
    {
      "epoch": 0.5232517236527368,
      "grad_norm": 8.505680084228516,
      "learning_rate": 1.6511655175648423e-05,
      "loss": 0.5525,
      "step": 59500
    },
    {
      "epoch": 0.527648796960743,
      "grad_norm": 2.4597957134246826,
      "learning_rate": 1.6482341353595048e-05,
      "loss": 0.5566,
      "step": 60000
    },
    {
      "epoch": 0.5320458702687492,
      "grad_norm": 4.619011402130127,
      "learning_rate": 1.6453027531541676e-05,
      "loss": 0.5605,
      "step": 60500
    },
    {
      "epoch": 0.5364429435767553,
      "grad_norm": 10.093810081481934,
      "learning_rate": 1.64237137094883e-05,
      "loss": 0.5482,
      "step": 61000
    },
    {
      "epoch": 0.5408400168847615,
      "grad_norm": 90.4350357055664,
      "learning_rate": 1.6394399887434925e-05,
      "loss": 0.5537,
      "step": 61500
    },
    {
      "epoch": 0.5452370901927677,
      "grad_norm": 10.22619915008545,
      "learning_rate": 1.636508606538155e-05,
      "loss": 0.5392,
      "step": 62000
    },
    {
      "epoch": 0.5496341635007739,
      "grad_norm": 3.843352794647217,
      "learning_rate": 1.6335772243328175e-05,
      "loss": 0.5457,
      "step": 62500
    },
    {
      "epoch": 0.5540312368087801,
      "grad_norm": 28.394519805908203,
      "learning_rate": 1.63064584212748e-05,
      "loss": 0.5326,
      "step": 63000
    },
    {
      "epoch": 0.5584283101167863,
      "grad_norm": 13.262115478515625,
      "learning_rate": 1.6277144599221425e-05,
      "loss": 0.5303,
      "step": 63500
    },
    {
      "epoch": 0.5628253834247925,
      "grad_norm": 1.493016004562378,
      "learning_rate": 1.6247830777168053e-05,
      "loss": 0.5313,
      "step": 64000
    },
    {
      "epoch": 0.5672224567327987,
      "grad_norm": 5.441362380981445,
      "learning_rate": 1.6218516955114674e-05,
      "loss": 0.5361,
      "step": 64500
    },
    {
      "epoch": 0.5716195300408048,
      "grad_norm": 56.641090393066406,
      "learning_rate": 1.6189203133061303e-05,
      "loss": 0.5437,
      "step": 65000
    },
    {
      "epoch": 0.576016603348811,
      "grad_norm": 61.777076721191406,
      "learning_rate": 1.6159889311007927e-05,
      "loss": 0.5739,
      "step": 65500
    },
    {
      "epoch": 0.5804136766568172,
      "grad_norm": 9.777847290039062,
      "learning_rate": 1.6130575488954556e-05,
      "loss": 0.5705,
      "step": 66000
    },
    {
      "epoch": 0.5848107499648234,
      "grad_norm": 6.483413219451904,
      "learning_rate": 1.6101261666901177e-05,
      "loss": 0.5554,
      "step": 66500
    },
    {
      "epoch": 0.5892078232728296,
      "grad_norm": 10.216031074523926,
      "learning_rate": 1.6071947844847805e-05,
      "loss": 0.5508,
      "step": 67000
    },
    {
      "epoch": 0.5936048965808358,
      "grad_norm": 2.597787857055664,
      "learning_rate": 1.604263402279443e-05,
      "loss": 0.5555,
      "step": 67500
    },
    {
      "epoch": 0.598001969888842,
      "grad_norm": 13.603822708129883,
      "learning_rate": 1.6013320200741055e-05,
      "loss": 0.5559,
      "step": 68000
    },
    {
      "epoch": 0.6023990431968482,
      "grad_norm": 8.096362113952637,
      "learning_rate": 1.598400637868768e-05,
      "loss": 0.5565,
      "step": 68500
    },
    {
      "epoch": 0.6067961165048543,
      "grad_norm": 3.56840443611145,
      "learning_rate": 1.5954692556634304e-05,
      "loss": 0.5516,
      "step": 69000
    },
    {
      "epoch": 0.6111931898128605,
      "grad_norm": 2.039788246154785,
      "learning_rate": 1.5925378734580933e-05,
      "loss": 0.5573,
      "step": 69500
    },
    {
      "epoch": 0.6155902631208667,
      "grad_norm": 1.5770798921585083,
      "learning_rate": 1.5896064912527554e-05,
      "loss": 0.5561,
      "step": 70000
    },
    {
      "epoch": 0.6199873364288729,
      "grad_norm": 4.879302501678467,
      "learning_rate": 1.5866751090474182e-05,
      "loss": 0.5448,
      "step": 70500
    },
    {
      "epoch": 0.6243844097368791,
      "grad_norm": 17.072219848632812,
      "learning_rate": 1.5837437268420807e-05,
      "loss": 0.5563,
      "step": 71000
    },
    {
      "epoch": 0.6287814830448853,
      "grad_norm": 10.506272315979004,
      "learning_rate": 1.5808123446367432e-05,
      "loss": 0.554,
      "step": 71500
    },
    {
      "epoch": 0.6331785563528916,
      "grad_norm": 3.6002604961395264,
      "learning_rate": 1.5778809624314057e-05,
      "loss": 0.5314,
      "step": 72000
    },
    {
      "epoch": 0.6375756296608978,
      "grad_norm": 3.7738916873931885,
      "learning_rate": 1.5749495802260685e-05,
      "loss": 0.5353,
      "step": 72500
    },
    {
      "epoch": 0.6419727029689039,
      "grad_norm": 6.61513614654541,
      "learning_rate": 1.572018198020731e-05,
      "loss": 0.5435,
      "step": 73000
    },
    {
      "epoch": 0.6463697762769101,
      "grad_norm": 1.9893622398376465,
      "learning_rate": 1.5690868158153935e-05,
      "loss": 0.5379,
      "step": 73500
    },
    {
      "epoch": 0.6507668495849163,
      "grad_norm": 2.388944149017334,
      "learning_rate": 1.566155433610056e-05,
      "loss": 0.5476,
      "step": 74000
    },
    {
      "epoch": 0.6551639228929225,
      "grad_norm": 5.160058498382568,
      "learning_rate": 1.5632240514047184e-05,
      "loss": 0.5467,
      "step": 74500
    },
    {
      "epoch": 0.6595609962009287,
      "grad_norm": 7.376406192779541,
      "learning_rate": 1.560292669199381e-05,
      "loss": 0.5448,
      "step": 75000
    },
    {
      "epoch": 0.6639580695089349,
      "grad_norm": 66.33251190185547,
      "learning_rate": 1.5573612869940434e-05,
      "loss": 0.5455,
      "step": 75500
    },
    {
      "epoch": 0.6683551428169411,
      "grad_norm": 2.8095974922180176,
      "learning_rate": 1.5544299047887062e-05,
      "loss": 0.5406,
      "step": 76000
    },
    {
      "epoch": 0.6727522161249473,
      "grad_norm": 10.146880149841309,
      "learning_rate": 1.5514985225833687e-05,
      "loss": 0.5459,
      "step": 76500
    },
    {
      "epoch": 0.6771492894329534,
      "grad_norm": 14.650792121887207,
      "learning_rate": 1.548567140378031e-05,
      "loss": 0.5595,
      "step": 77000
    },
    {
      "epoch": 0.6815463627409596,
      "grad_norm": 6.8442840576171875,
      "learning_rate": 1.5456357581726937e-05,
      "loss": 0.5391,
      "step": 77500
    },
    {
      "epoch": 0.6859434360489658,
      "grad_norm": 7.056739330291748,
      "learning_rate": 1.5427043759673565e-05,
      "loss": 0.5466,
      "step": 78000
    },
    {
      "epoch": 0.690340509356972,
      "grad_norm": 5.539918422698975,
      "learning_rate": 1.5397729937620186e-05,
      "loss": 0.552,
      "step": 78500
    },
    {
      "epoch": 0.6947375826649782,
      "grad_norm": 4.7791428565979,
      "learning_rate": 1.5368416115566814e-05,
      "loss": 0.5318,
      "step": 79000
    },
    {
      "epoch": 0.6991346559729844,
      "grad_norm": 37.21333312988281,
      "learning_rate": 1.533910229351344e-05,
      "loss": 0.5531,
      "step": 79500
    },
    {
      "epoch": 0.7035317292809906,
      "grad_norm": 6.087508201599121,
      "learning_rate": 1.5309788471460064e-05,
      "loss": 0.5349,
      "step": 80000
    },
    {
      "epoch": 0.7079288025889967,
      "grad_norm": 3.572744369506836,
      "learning_rate": 1.528047464940669e-05,
      "loss": 0.5427,
      "step": 80500
    },
    {
      "epoch": 0.7123258758970029,
      "grad_norm": 9.932648658752441,
      "learning_rate": 1.5251160827353315e-05,
      "loss": 0.5669,
      "step": 81000
    },
    {
      "epoch": 0.7167229492050091,
      "grad_norm": 7.024425983428955,
      "learning_rate": 1.5221847005299942e-05,
      "loss": 0.5705,
      "step": 81500
    },
    {
      "epoch": 0.7211200225130153,
      "grad_norm": 7.045307636260986,
      "learning_rate": 1.5192533183246565e-05,
      "loss": 0.5474,
      "step": 82000
    },
    {
      "epoch": 0.7255170958210215,
      "grad_norm": 5.152991771697998,
      "learning_rate": 1.5163219361193191e-05,
      "loss": 0.5562,
      "step": 82500
    },
    {
      "epoch": 0.7299141691290277,
      "grad_norm": 18.036466598510742,
      "learning_rate": 1.5133905539139816e-05,
      "loss": 0.5354,
      "step": 83000
    },
    {
      "epoch": 0.7343112424370339,
      "grad_norm": 1.9240756034851074,
      "learning_rate": 1.5104591717086441e-05,
      "loss": 0.5671,
      "step": 83500
    },
    {
      "epoch": 0.7387083157450401,
      "grad_norm": 6.204875946044922,
      "learning_rate": 1.5075277895033068e-05,
      "loss": 0.5646,
      "step": 84000
    },
    {
      "epoch": 0.7431053890530462,
      "grad_norm": 8.217890739440918,
      "learning_rate": 1.5045964072979692e-05,
      "loss": 0.5405,
      "step": 84500
    },
    {
      "epoch": 0.7475024623610524,
      "grad_norm": 4.4240217208862305,
      "learning_rate": 1.5016650250926319e-05,
      "loss": 0.5788,
      "step": 85000
    },
    {
      "epoch": 0.7518995356690586,
      "grad_norm": 8.42997932434082,
      "learning_rate": 1.4987336428872942e-05,
      "loss": 0.5328,
      "step": 85500
    },
    {
      "epoch": 0.7562966089770649,
      "grad_norm": 4.968165397644043,
      "learning_rate": 1.4958022606819569e-05,
      "loss": 0.5419,
      "step": 86000
    },
    {
      "epoch": 0.7606936822850711,
      "grad_norm": 7.935583114624023,
      "learning_rate": 1.4928708784766195e-05,
      "loss": 0.541,
      "step": 86500
    },
    {
      "epoch": 0.7650907555930773,
      "grad_norm": 32.832698822021484,
      "learning_rate": 1.4899394962712818e-05,
      "loss": 0.5438,
      "step": 87000
    },
    {
      "epoch": 0.7694878289010835,
      "grad_norm": 107.09083557128906,
      "learning_rate": 1.4870081140659445e-05,
      "loss": 0.5349,
      "step": 87500
    },
    {
      "epoch": 0.7738849022090897,
      "grad_norm": 7.383637428283691,
      "learning_rate": 1.4840767318606071e-05,
      "loss": 0.5554,
      "step": 88000
    },
    {
      "epoch": 0.7782819755170958,
      "grad_norm": 9.894816398620605,
      "learning_rate": 1.4811453496552696e-05,
      "loss": 0.5325,
      "step": 88500
    },
    {
      "epoch": 0.782679048825102,
      "grad_norm": 8.737370491027832,
      "learning_rate": 1.4782139674499321e-05,
      "loss": 0.5345,
      "step": 89000
    },
    {
      "epoch": 0.7870761221331082,
      "grad_norm": 4.829423904418945,
      "learning_rate": 1.4752825852445947e-05,
      "loss": 0.5299,
      "step": 89500
    },
    {
      "epoch": 0.7914731954411144,
      "grad_norm": 5.346202373504639,
      "learning_rate": 1.4723512030392572e-05,
      "loss": 0.5581,
      "step": 90000
    },
    {
      "epoch": 0.7958702687491206,
      "grad_norm": 52.38899230957031,
      "learning_rate": 1.4694198208339197e-05,
      "loss": 0.5279,
      "step": 90500
    },
    {
      "epoch": 0.8002673420571268,
      "grad_norm": 3.337007522583008,
      "learning_rate": 1.4664884386285822e-05,
      "loss": 0.5262,
      "step": 91000
    },
    {
      "epoch": 0.804664415365133,
      "grad_norm": 6.98732328414917,
      "learning_rate": 1.4635570564232448e-05,
      "loss": 0.5362,
      "step": 91500
    },
    {
      "epoch": 0.8090614886731392,
      "grad_norm": 6.12302827835083,
      "learning_rate": 1.4606256742179073e-05,
      "loss": 0.5353,
      "step": 92000
    },
    {
      "epoch": 0.8134585619811453,
      "grad_norm": 4.585868835449219,
      "learning_rate": 1.4576942920125698e-05,
      "loss": 0.533,
      "step": 92500
    },
    {
      "epoch": 0.8178556352891515,
      "grad_norm": 2.313876152038574,
      "learning_rate": 1.4547629098072324e-05,
      "loss": 0.562,
      "step": 93000
    },
    {
      "epoch": 0.8222527085971577,
      "grad_norm": 97.51795959472656,
      "learning_rate": 1.4518315276018951e-05,
      "loss": 0.5678,
      "step": 93500
    },
    {
      "epoch": 0.8266497819051639,
      "grad_norm": 17.308008193969727,
      "learning_rate": 1.4489001453965574e-05,
      "loss": 0.5494,
      "step": 94000
    },
    {
      "epoch": 0.8310468552131701,
      "grad_norm": 6.365790367126465,
      "learning_rate": 1.44596876319122e-05,
      "loss": 0.5543,
      "step": 94500
    },
    {
      "epoch": 0.8354439285211763,
      "grad_norm": 62.36254119873047,
      "learning_rate": 1.4430373809858827e-05,
      "loss": 0.5329,
      "step": 95000
    },
    {
      "epoch": 0.8398410018291825,
      "grad_norm": 7.853355407714844,
      "learning_rate": 1.440105998780545e-05,
      "loss": 0.528,
      "step": 95500
    },
    {
      "epoch": 0.8442380751371887,
      "grad_norm": 2.999161720275879,
      "learning_rate": 1.4371746165752077e-05,
      "loss": 0.5336,
      "step": 96000
    },
    {
      "epoch": 0.8486351484451948,
      "grad_norm": 1.729049563407898,
      "learning_rate": 1.4342432343698702e-05,
      "loss": 0.5233,
      "step": 96500
    },
    {
      "epoch": 0.853032221753201,
      "grad_norm": 4.152891635894775,
      "learning_rate": 1.4313118521645328e-05,
      "loss": 0.5558,
      "step": 97000
    },
    {
      "epoch": 0.8574292950612072,
      "grad_norm": 8.22642707824707,
      "learning_rate": 1.4283804699591953e-05,
      "loss": 0.56,
      "step": 97500
    },
    {
      "epoch": 0.8618263683692134,
      "grad_norm": 87.15044403076172,
      "learning_rate": 1.4254490877538578e-05,
      "loss": 0.5415,
      "step": 98000
    },
    {
      "epoch": 0.8662234416772197,
      "grad_norm": 15.650408744812012,
      "learning_rate": 1.4225177055485204e-05,
      "loss": 0.5661,
      "step": 98500
    },
    {
      "epoch": 0.8706205149852259,
      "grad_norm": 13.030996322631836,
      "learning_rate": 1.4195863233431827e-05,
      "loss": 0.5483,
      "step": 99000
    },
    {
      "epoch": 0.8750175882932321,
      "grad_norm": 16.422826766967773,
      "learning_rate": 1.4166549411378454e-05,
      "loss": 0.5579,
      "step": 99500
    },
    {
      "epoch": 0.8794146616012383,
      "grad_norm": 3.974818468093872,
      "learning_rate": 1.413723558932508e-05,
      "loss": 0.5524,
      "step": 100000
    },
    {
      "epoch": 0.8838117349092444,
      "grad_norm": 67.67200469970703,
      "learning_rate": 1.4107921767271707e-05,
      "loss": 0.544,
      "step": 100500
    },
    {
      "epoch": 0.8882088082172506,
      "grad_norm": 3.324514865875244,
      "learning_rate": 1.407860794521833e-05,
      "loss": 0.5649,
      "step": 101000
    },
    {
      "epoch": 0.8926058815252568,
      "grad_norm": 2.4737792015075684,
      "learning_rate": 1.4049294123164957e-05,
      "loss": 0.5477,
      "step": 101500
    },
    {
      "epoch": 0.897002954833263,
      "grad_norm": 13.133896827697754,
      "learning_rate": 1.4019980301111581e-05,
      "loss": 0.5301,
      "step": 102000
    },
    {
      "epoch": 0.9014000281412692,
      "grad_norm": 8.610488891601562,
      "learning_rate": 1.3990666479058206e-05,
      "loss": 0.5324,
      "step": 102500
    },
    {
      "epoch": 0.9057971014492754,
      "grad_norm": 8.272149085998535,
      "learning_rate": 1.3961352657004833e-05,
      "loss": 0.555,
      "step": 103000
    },
    {
      "epoch": 0.9101941747572816,
      "grad_norm": 8.298799514770508,
      "learning_rate": 1.3932038834951458e-05,
      "loss": 0.5346,
      "step": 103500
    },
    {
      "epoch": 0.9145912480652878,
      "grad_norm": 105.24739837646484,
      "learning_rate": 1.3902725012898082e-05,
      "loss": 0.534,
      "step": 104000
    },
    {
      "epoch": 0.9189883213732939,
      "grad_norm": 1.9403334856033325,
      "learning_rate": 1.3873411190844707e-05,
      "loss": 0.5341,
      "step": 104500
    },
    {
      "epoch": 0.9233853946813001,
      "grad_norm": 3.193643569946289,
      "learning_rate": 1.3844097368791334e-05,
      "loss": 0.5388,
      "step": 105000
    },
    {
      "epoch": 0.9277824679893063,
      "grad_norm": 1.4785889387130737,
      "learning_rate": 1.381478354673796e-05,
      "loss": 0.5326,
      "step": 105500
    },
    {
      "epoch": 0.9321795412973125,
      "grad_norm": 5.860851764678955,
      "learning_rate": 1.3785469724684583e-05,
      "loss": 0.5482,
      "step": 106000
    },
    {
      "epoch": 0.9365766146053187,
      "grad_norm": 38.235618591308594,
      "learning_rate": 1.375615590263121e-05,
      "loss": 0.5265,
      "step": 106500
    },
    {
      "epoch": 0.9409736879133249,
      "grad_norm": 3.556523084640503,
      "learning_rate": 1.3726842080577836e-05,
      "loss": 0.5353,
      "step": 107000
    },
    {
      "epoch": 0.9453707612213311,
      "grad_norm": 12.311854362487793,
      "learning_rate": 1.369752825852446e-05,
      "loss": 0.5242,
      "step": 107500
    },
    {
      "epoch": 0.9497678345293372,
      "grad_norm": 2.64849853515625,
      "learning_rate": 1.3668214436471086e-05,
      "loss": 0.5471,
      "step": 108000
    },
    {
      "epoch": 0.9541649078373434,
      "grad_norm": 8.624571800231934,
      "learning_rate": 1.363890061441771e-05,
      "loss": 0.5382,
      "step": 108500
    },
    {
      "epoch": 0.9585619811453496,
      "grad_norm": 4.15562629699707,
      "learning_rate": 1.3609586792364337e-05,
      "loss": 0.5222,
      "step": 109000
    },
    {
      "epoch": 0.9629590544533558,
      "grad_norm": 3.0646257400512695,
      "learning_rate": 1.3580272970310962e-05,
      "loss": 0.5401,
      "step": 109500
    },
    {
      "epoch": 0.967356127761362,
      "grad_norm": 7.104803085327148,
      "learning_rate": 1.3550959148257587e-05,
      "loss": 0.5345,
      "step": 110000
    },
    {
      "epoch": 0.9717532010693682,
      "grad_norm": 11.038168907165527,
      "learning_rate": 1.3521645326204213e-05,
      "loss": 0.5497,
      "step": 110500
    },
    {
      "epoch": 0.9761502743773744,
      "grad_norm": 96.446533203125,
      "learning_rate": 1.3492331504150837e-05,
      "loss": 0.5412,
      "step": 111000
    },
    {
      "epoch": 0.9805473476853807,
      "grad_norm": 34.697265625,
      "learning_rate": 1.3463017682097463e-05,
      "loss": 0.5353,
      "step": 111500
    },
    {
      "epoch": 0.9849444209933867,
      "grad_norm": 5.343205451965332,
      "learning_rate": 1.343370386004409e-05,
      "loss": 0.5328,
      "step": 112000
    },
    {
      "epoch": 0.989341494301393,
      "grad_norm": 3.607635259628296,
      "learning_rate": 1.3404390037990716e-05,
      "loss": 0.5298,
      "step": 112500
    },
    {
      "epoch": 0.9937385676093992,
      "grad_norm": 14.063156127929688,
      "learning_rate": 1.337507621593734e-05,
      "loss": 0.5308,
      "step": 113000
    },
    {
      "epoch": 0.9981356409174054,
      "grad_norm": 64.68863677978516,
      "learning_rate": 1.3345762393883966e-05,
      "loss": 0.528,
      "step": 113500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7535368725142959,
      "eval_f1": 0.7474452554744525,
      "eval_loss": 0.5419355034828186,
      "eval_precision": 0.7663555287611999,
      "eval_recall": 0.7294457526168946,
      "eval_runtime": 42.3608,
      "eval_samples_per_second": 2386.119,
      "eval_steps_per_second": 149.147,
      "step": 113712
    },
    {
      "epoch": 1.0025327142254115,
      "grad_norm": 28.011808395385742,
      "learning_rate": 1.331644857183059e-05,
      "loss": 0.5149,
      "step": 114000
    },
    {
      "epoch": 1.0069297875334178,
      "grad_norm": 9.25292682647705,
      "learning_rate": 1.3287134749777215e-05,
      "loss": 0.5224,
      "step": 114500
    },
    {
      "epoch": 1.0113268608414239,
      "grad_norm": 7.566316604614258,
      "learning_rate": 1.3257820927723842e-05,
      "loss": 0.525,
      "step": 115000
    },
    {
      "epoch": 1.0157239341494302,
      "grad_norm": 7.31606912612915,
      "learning_rate": 1.3228507105670467e-05,
      "loss": 0.5088,
      "step": 115500
    },
    {
      "epoch": 1.0201210074574363,
      "grad_norm": 9.821502685546875,
      "learning_rate": 1.3199193283617093e-05,
      "loss": 0.5032,
      "step": 116000
    },
    {
      "epoch": 1.0245180807654426,
      "grad_norm": 10.544501304626465,
      "learning_rate": 1.3169879461563716e-05,
      "loss": 0.5102,
      "step": 116500
    },
    {
      "epoch": 1.0289151540734487,
      "grad_norm": 5.048376083374023,
      "learning_rate": 1.3140565639510343e-05,
      "loss": 0.511,
      "step": 117000
    },
    {
      "epoch": 1.033312227381455,
      "grad_norm": 10.037517547607422,
      "learning_rate": 1.311125181745697e-05,
      "loss": 0.5102,
      "step": 117500
    },
    {
      "epoch": 1.037709300689461,
      "grad_norm": 5.872483730316162,
      "learning_rate": 1.3081937995403592e-05,
      "loss": 0.5203,
      "step": 118000
    },
    {
      "epoch": 1.0421063739974672,
      "grad_norm": 6.630013942718506,
      "learning_rate": 1.3052624173350219e-05,
      "loss": 0.525,
      "step": 118500
    },
    {
      "epoch": 1.0465034473054735,
      "grad_norm": 4.89944314956665,
      "learning_rate": 1.3023310351296846e-05,
      "loss": 0.5115,
      "step": 119000
    },
    {
      "epoch": 1.0509005206134796,
      "grad_norm": 5.996119976043701,
      "learning_rate": 1.2993996529243469e-05,
      "loss": 0.5037,
      "step": 119500
    },
    {
      "epoch": 1.055297593921486,
      "grad_norm": 4.534438610076904,
      "learning_rate": 1.2964682707190095e-05,
      "loss": 0.5149,
      "step": 120000
    },
    {
      "epoch": 1.059694667229492,
      "grad_norm": 7.261202812194824,
      "learning_rate": 1.2935368885136722e-05,
      "loss": 0.5078,
      "step": 120500
    },
    {
      "epoch": 1.0640917405374983,
      "grad_norm": 4.497305393218994,
      "learning_rate": 1.2906055063083346e-05,
      "loss": 0.5001,
      "step": 121000
    },
    {
      "epoch": 1.0684888138455044,
      "grad_norm": 2.7046992778778076,
      "learning_rate": 1.2876741241029971e-05,
      "loss": 0.5121,
      "step": 121500
    },
    {
      "epoch": 1.0728858871535105,
      "grad_norm": 6.929440975189209,
      "learning_rate": 1.2847427418976596e-05,
      "loss": 0.5126,
      "step": 122000
    },
    {
      "epoch": 1.0772829604615168,
      "grad_norm": 4.102485656738281,
      "learning_rate": 1.2818113596923223e-05,
      "loss": 0.5173,
      "step": 122500
    },
    {
      "epoch": 1.081680033769523,
      "grad_norm": 58.924041748046875,
      "learning_rate": 1.2788799774869847e-05,
      "loss": 0.5049,
      "step": 123000
    },
    {
      "epoch": 1.0860771070775292,
      "grad_norm": 5.690413475036621,
      "learning_rate": 1.2759485952816472e-05,
      "loss": 0.5105,
      "step": 123500
    },
    {
      "epoch": 1.0904741803855353,
      "grad_norm": 8.842035293579102,
      "learning_rate": 1.2730172130763099e-05,
      "loss": 0.5105,
      "step": 124000
    },
    {
      "epoch": 1.0948712536935417,
      "grad_norm": 2.9201533794403076,
      "learning_rate": 1.2700858308709725e-05,
      "loss": 0.5055,
      "step": 124500
    },
    {
      "epoch": 1.0992683270015478,
      "grad_norm": 38.93973922729492,
      "learning_rate": 1.2671544486656348e-05,
      "loss": 0.6315,
      "step": 125000
    },
    {
      "epoch": 1.1036654003095538,
      "grad_norm": 972.0725708007812,
      "learning_rate": 1.2642230664602975e-05,
      "loss": 0.5792,
      "step": 125500
    },
    {
      "epoch": 1.1080624736175602,
      "grad_norm": 53.2138671875,
      "learning_rate": 1.2612916842549601e-05,
      "loss": 0.5302,
      "step": 126000
    },
    {
      "epoch": 1.1124595469255663,
      "grad_norm": 9.085047721862793,
      "learning_rate": 1.2583603020496225e-05,
      "loss": 0.4932,
      "step": 126500
    },
    {
      "epoch": 1.1168566202335726,
      "grad_norm": 78.36217498779297,
      "learning_rate": 1.2554289198442851e-05,
      "loss": 0.5078,
      "step": 127000
    },
    {
      "epoch": 1.1212536935415787,
      "grad_norm": 5.286334991455078,
      "learning_rate": 1.2524975376389476e-05,
      "loss": 0.5078,
      "step": 127500
    },
    {
      "epoch": 1.125650766849585,
      "grad_norm": 2.9507594108581543,
      "learning_rate": 1.2495661554336102e-05,
      "loss": 0.5166,
      "step": 128000
    },
    {
      "epoch": 1.130047840157591,
      "grad_norm": 9.277560234069824,
      "learning_rate": 1.2466347732282727e-05,
      "loss": 0.5187,
      "step": 128500
    },
    {
      "epoch": 1.1344449134655974,
      "grad_norm": 7.2339982986450195,
      "learning_rate": 1.2437033910229352e-05,
      "loss": 0.5104,
      "step": 129000
    },
    {
      "epoch": 1.1388419867736035,
      "grad_norm": 69.6374740600586,
      "learning_rate": 1.2407720088175979e-05,
      "loss": 0.5182,
      "step": 129500
    },
    {
      "epoch": 1.1432390600816098,
      "grad_norm": 3.1364803314208984,
      "learning_rate": 1.2378406266122602e-05,
      "loss": 0.507,
      "step": 130000
    },
    {
      "epoch": 1.147636133389616,
      "grad_norm": 5.565731048583984,
      "learning_rate": 1.2349092444069228e-05,
      "loss": 0.5083,
      "step": 130500
    },
    {
      "epoch": 1.152033206697622,
      "grad_norm": 17.563419342041016,
      "learning_rate": 1.2319778622015855e-05,
      "loss": 0.4915,
      "step": 131000
    },
    {
      "epoch": 1.1564302800056283,
      "grad_norm": 24.933979034423828,
      "learning_rate": 1.2290464799962478e-05,
      "loss": 0.5025,
      "step": 131500
    },
    {
      "epoch": 1.1608273533136344,
      "grad_norm": 11.077561378479004,
      "learning_rate": 1.2261150977909104e-05,
      "loss": 0.51,
      "step": 132000
    },
    {
      "epoch": 1.1652244266216407,
      "grad_norm": 4.70433235168457,
      "learning_rate": 1.223183715585573e-05,
      "loss": 0.5141,
      "step": 132500
    },
    {
      "epoch": 1.1696214999296468,
      "grad_norm": 9.07050895690918,
      "learning_rate": 1.2202523333802356e-05,
      "loss": 0.5072,
      "step": 133000
    },
    {
      "epoch": 1.1740185732376531,
      "grad_norm": 6.216267108917236,
      "learning_rate": 1.217320951174898e-05,
      "loss": 0.5121,
      "step": 133500
    },
    {
      "epoch": 1.1784156465456592,
      "grad_norm": 5.765195369720459,
      "learning_rate": 1.2143895689695607e-05,
      "loss": 0.5268,
      "step": 134000
    },
    {
      "epoch": 1.1828127198536653,
      "grad_norm": 12.39975643157959,
      "learning_rate": 1.2114581867642232e-05,
      "loss": 0.4961,
      "step": 134500
    },
    {
      "epoch": 1.1872097931616716,
      "grad_norm": 2.46341872215271,
      "learning_rate": 1.2085268045588857e-05,
      "loss": 0.4986,
      "step": 135000
    },
    {
      "epoch": 1.1916068664696777,
      "grad_norm": 3.4116814136505127,
      "learning_rate": 1.2055954223535481e-05,
      "loss": 0.4967,
      "step": 135500
    },
    {
      "epoch": 1.196003939777684,
      "grad_norm": 4.977638244628906,
      "learning_rate": 1.2026640401482108e-05,
      "loss": 0.4956,
      "step": 136000
    },
    {
      "epoch": 1.2004010130856901,
      "grad_norm": 7.452854633331299,
      "learning_rate": 1.1997326579428734e-05,
      "loss": 0.4964,
      "step": 136500
    },
    {
      "epoch": 1.2047980863936965,
      "grad_norm": 5.1318535804748535,
      "learning_rate": 1.1968012757375358e-05,
      "loss": 0.5028,
      "step": 137000
    },
    {
      "epoch": 1.2091951597017025,
      "grad_norm": 8.885765075683594,
      "learning_rate": 1.1938698935321984e-05,
      "loss": 0.5141,
      "step": 137500
    },
    {
      "epoch": 1.2135922330097086,
      "grad_norm": 7.119990825653076,
      "learning_rate": 1.190938511326861e-05,
      "loss": 0.4912,
      "step": 138000
    },
    {
      "epoch": 1.217989306317715,
      "grad_norm": 6.599311351776123,
      "learning_rate": 1.1880071291215234e-05,
      "loss": 0.5109,
      "step": 138500
    },
    {
      "epoch": 1.222386379625721,
      "grad_norm": 3.853064775466919,
      "learning_rate": 1.185075746916186e-05,
      "loss": 0.5135,
      "step": 139000
    },
    {
      "epoch": 1.2267834529337274,
      "grad_norm": 4.545908451080322,
      "learning_rate": 1.1821443647108485e-05,
      "loss": 0.5166,
      "step": 139500
    },
    {
      "epoch": 1.2311805262417335,
      "grad_norm": 15.42772388458252,
      "learning_rate": 1.1792129825055112e-05,
      "loss": 0.4931,
      "step": 140000
    },
    {
      "epoch": 1.2355775995497398,
      "grad_norm": 6.839496612548828,
      "learning_rate": 1.1762816003001736e-05,
      "loss": 0.522,
      "step": 140500
    },
    {
      "epoch": 1.2399746728577459,
      "grad_norm": 3.830897569656372,
      "learning_rate": 1.1733502180948361e-05,
      "loss": 0.502,
      "step": 141000
    },
    {
      "epoch": 1.244371746165752,
      "grad_norm": 11.763043403625488,
      "learning_rate": 1.1704188358894988e-05,
      "loss": 0.5044,
      "step": 141500
    },
    {
      "epoch": 1.2487688194737583,
      "grad_norm": 4.736271858215332,
      "learning_rate": 1.1674874536841611e-05,
      "loss": 0.5099,
      "step": 142000
    },
    {
      "epoch": 1.2531658927817644,
      "grad_norm": 7.190158367156982,
      "learning_rate": 1.1645560714788237e-05,
      "loss": 0.4967,
      "step": 142500
    },
    {
      "epoch": 1.2575629660897707,
      "grad_norm": 24.03020668029785,
      "learning_rate": 1.1616246892734864e-05,
      "loss": 0.5121,
      "step": 143000
    },
    {
      "epoch": 1.2619600393977768,
      "grad_norm": 4.56619119644165,
      "learning_rate": 1.158693307068149e-05,
      "loss": 0.4939,
      "step": 143500
    },
    {
      "epoch": 1.266357112705783,
      "grad_norm": 9.435276985168457,
      "learning_rate": 1.1557619248628114e-05,
      "loss": 0.4884,
      "step": 144000
    },
    {
      "epoch": 1.2707541860137892,
      "grad_norm": 11.603629112243652,
      "learning_rate": 1.152830542657474e-05,
      "loss": 0.4894,
      "step": 144500
    },
    {
      "epoch": 1.2751512593217953,
      "grad_norm": 9.144611358642578,
      "learning_rate": 1.1498991604521365e-05,
      "loss": 0.5015,
      "step": 145000
    },
    {
      "epoch": 1.2795483326298016,
      "grad_norm": 5.699421405792236,
      "learning_rate": 1.146967778246799e-05,
      "loss": 0.5061,
      "step": 145500
    },
    {
      "epoch": 1.283945405937808,
      "grad_norm": 16.04778480529785,
      "learning_rate": 1.1440363960414616e-05,
      "loss": 0.506,
      "step": 146000
    },
    {
      "epoch": 1.288342479245814,
      "grad_norm": 3.2275516986846924,
      "learning_rate": 1.1411050138361241e-05,
      "loss": 0.5122,
      "step": 146500
    },
    {
      "epoch": 1.2927395525538201,
      "grad_norm": 7.668974876403809,
      "learning_rate": 1.1381736316307866e-05,
      "loss": 0.4966,
      "step": 147000
    },
    {
      "epoch": 1.2971366258618264,
      "grad_norm": 8.372355461120605,
      "learning_rate": 1.135242249425449e-05,
      "loss": 0.4982,
      "step": 147500
    },
    {
      "epoch": 1.3015336991698325,
      "grad_norm": 10.320610046386719,
      "learning_rate": 1.1323108672201117e-05,
      "loss": 0.4986,
      "step": 148000
    },
    {
      "epoch": 1.3059307724778386,
      "grad_norm": 9.506994247436523,
      "learning_rate": 1.1293794850147744e-05,
      "loss": 0.4943,
      "step": 148500
    },
    {
      "epoch": 1.310327845785845,
      "grad_norm": 6.450611114501953,
      "learning_rate": 1.1264481028094367e-05,
      "loss": 0.4976,
      "step": 149000
    },
    {
      "epoch": 1.3147249190938513,
      "grad_norm": 36.20949172973633,
      "learning_rate": 1.1235167206040993e-05,
      "loss": 0.6285,
      "step": 149500
    },
    {
      "epoch": 1.3191219924018573,
      "grad_norm": 76.34686279296875,
      "learning_rate": 1.120585338398762e-05,
      "loss": 0.5175,
      "step": 150000
    },
    {
      "epoch": 1.3235190657098634,
      "grad_norm": 1.99996817111969,
      "learning_rate": 1.1176539561934243e-05,
      "loss": 0.5239,
      "step": 150500
    },
    {
      "epoch": 1.3279161390178698,
      "grad_norm": 15.085034370422363,
      "learning_rate": 1.114722573988087e-05,
      "loss": 0.5273,
      "step": 151000
    },
    {
      "epoch": 1.3323132123258759,
      "grad_norm": 18.077356338500977,
      "learning_rate": 1.1117911917827496e-05,
      "loss": 0.5154,
      "step": 151500
    },
    {
      "epoch": 1.3367102856338822,
      "grad_norm": 5.808826446533203,
      "learning_rate": 1.108859809577412e-05,
      "loss": 0.4859,
      "step": 152000
    },
    {
      "epoch": 1.3411073589418883,
      "grad_norm": 12.023737907409668,
      "learning_rate": 1.1059284273720746e-05,
      "loss": 0.4921,
      "step": 152500
    },
    {
      "epoch": 1.3455044322498946,
      "grad_norm": 5.447109699249268,
      "learning_rate": 1.102997045166737e-05,
      "loss": 0.5069,
      "step": 153000
    },
    {
      "epoch": 1.3499015055579007,
      "grad_norm": 7.309390068054199,
      "learning_rate": 1.1000656629613997e-05,
      "loss": 0.5232,
      "step": 153500
    },
    {
      "epoch": 1.3542985788659068,
      "grad_norm": 10.134720802307129,
      "learning_rate": 1.0971342807560622e-05,
      "loss": 0.5094,
      "step": 154000
    },
    {
      "epoch": 1.358695652173913,
      "grad_norm": 6.409681797027588,
      "learning_rate": 1.0942028985507247e-05,
      "loss": 0.4928,
      "step": 154500
    },
    {
      "epoch": 1.3630927254819192,
      "grad_norm": 4.033295154571533,
      "learning_rate": 1.0912715163453873e-05,
      "loss": 0.4863,
      "step": 155000
    },
    {
      "epoch": 1.3674897987899255,
      "grad_norm": 7.643619060516357,
      "learning_rate": 1.08834013414005e-05,
      "loss": 0.5111,
      "step": 155500
    },
    {
      "epoch": 1.3718868720979316,
      "grad_norm": 3.745112419128418,
      "learning_rate": 1.0854087519347123e-05,
      "loss": 0.4977,
      "step": 156000
    },
    {
      "epoch": 1.376283945405938,
      "grad_norm": 8.695550918579102,
      "learning_rate": 1.082477369729375e-05,
      "loss": 0.4981,
      "step": 156500
    },
    {
      "epoch": 1.380681018713944,
      "grad_norm": 10.942185401916504,
      "learning_rate": 1.0795459875240376e-05,
      "loss": 0.5031,
      "step": 157000
    },
    {
      "epoch": 1.38507809202195,
      "grad_norm": 3.7191004753112793,
      "learning_rate": 1.0766146053186999e-05,
      "loss": 0.4923,
      "step": 157500
    },
    {
      "epoch": 1.3894751653299564,
      "grad_norm": 7.948395729064941,
      "learning_rate": 1.0736832231133625e-05,
      "loss": 0.5017,
      "step": 158000
    },
    {
      "epoch": 1.3938722386379625,
      "grad_norm": 9.06102180480957,
      "learning_rate": 1.070751840908025e-05,
      "loss": 0.4871,
      "step": 158500
    },
    {
      "epoch": 1.3982693119459688,
      "grad_norm": 2.2052178382873535,
      "learning_rate": 1.0678204587026875e-05,
      "loss": 0.4986,
      "step": 159000
    },
    {
      "epoch": 1.402666385253975,
      "grad_norm": 3.9022843837738037,
      "learning_rate": 1.0648890764973502e-05,
      "loss": 0.5012,
      "step": 159500
    },
    {
      "epoch": 1.4070634585619812,
      "grad_norm": 7.237349033355713,
      "learning_rate": 1.0619576942920126e-05,
      "loss": 0.4788,
      "step": 160000
    },
    {
      "epoch": 1.4114605318699873,
      "grad_norm": 30.23628807067871,
      "learning_rate": 1.0590263120866753e-05,
      "loss": 0.4973,
      "step": 160500
    },
    {
      "epoch": 1.4158576051779934,
      "grad_norm": 12.237936973571777,
      "learning_rate": 1.0560949298813376e-05,
      "loss": 0.4946,
      "step": 161000
    },
    {
      "epoch": 1.4202546784859997,
      "grad_norm": 6.077822685241699,
      "learning_rate": 1.0531635476760002e-05,
      "loss": 0.4988,
      "step": 161500
    },
    {
      "epoch": 1.4246517517940058,
      "grad_norm": 3.4345152378082275,
      "learning_rate": 1.0502321654706629e-05,
      "loss": 0.5114,
      "step": 162000
    },
    {
      "epoch": 1.4290488251020121,
      "grad_norm": 5.7656145095825195,
      "learning_rate": 1.0473007832653252e-05,
      "loss": 0.5134,
      "step": 162500
    },
    {
      "epoch": 1.4334458984100182,
      "grad_norm": 4.678593635559082,
      "learning_rate": 1.0443694010599879e-05,
      "loss": 0.4827,
      "step": 163000
    },
    {
      "epoch": 1.4378429717180246,
      "grad_norm": 7.075407981872559,
      "learning_rate": 1.0414380188546505e-05,
      "loss": 0.5017,
      "step": 163500
    },
    {
      "epoch": 1.4422400450260306,
      "grad_norm": 7.7863874435424805,
      "learning_rate": 1.038506636649313e-05,
      "loss": 0.5075,
      "step": 164000
    },
    {
      "epoch": 1.4466371183340367,
      "grad_norm": 13.271397590637207,
      "learning_rate": 1.0355752544439755e-05,
      "loss": 0.4851,
      "step": 164500
    },
    {
      "epoch": 1.451034191642043,
      "grad_norm": 5.664182662963867,
      "learning_rate": 1.0326438722386381e-05,
      "loss": 0.4986,
      "step": 165000
    },
    {
      "epoch": 1.4554312649500494,
      "grad_norm": 5.642025947570801,
      "learning_rate": 1.0297124900333006e-05,
      "loss": 0.4886,
      "step": 165500
    },
    {
      "epoch": 1.4598283382580555,
      "grad_norm": 10.63672161102295,
      "learning_rate": 1.0267811078279631e-05,
      "loss": 0.4823,
      "step": 166000
    },
    {
      "epoch": 1.4642254115660616,
      "grad_norm": 83.051025390625,
      "learning_rate": 1.0238497256226256e-05,
      "loss": 0.5065,
      "step": 166500
    },
    {
      "epoch": 1.4686224848740679,
      "grad_norm": 5.114267826080322,
      "learning_rate": 1.0209183434172882e-05,
      "loss": 0.4947,
      "step": 167000
    },
    {
      "epoch": 1.473019558182074,
      "grad_norm": 8.882201194763184,
      "learning_rate": 1.0179869612119509e-05,
      "loss": 0.4941,
      "step": 167500
    },
    {
      "epoch": 1.47741663149008,
      "grad_norm": 8.212349891662598,
      "learning_rate": 1.0150555790066132e-05,
      "loss": 0.4986,
      "step": 168000
    },
    {
      "epoch": 1.4818137047980864,
      "grad_norm": 10.593549728393555,
      "learning_rate": 1.0121241968012758e-05,
      "loss": 0.4826,
      "step": 168500
    },
    {
      "epoch": 1.4862107781060927,
      "grad_norm": 18.07955551147461,
      "learning_rate": 1.0091928145959385e-05,
      "loss": 0.4861,
      "step": 169000
    },
    {
      "epoch": 1.4906078514140988,
      "grad_norm": 8.5948486328125,
      "learning_rate": 1.0062614323906008e-05,
      "loss": 0.4879,
      "step": 169500
    },
    {
      "epoch": 1.4950049247221049,
      "grad_norm": 12.549747467041016,
      "learning_rate": 1.0033300501852635e-05,
      "loss": 0.4841,
      "step": 170000
    },
    {
      "epoch": 1.4994019980301112,
      "grad_norm": 5.593748092651367,
      "learning_rate": 1.000398667979926e-05,
      "loss": 0.503,
      "step": 170500
    },
    {
      "epoch": 1.5037990713381173,
      "grad_norm": 4.865285396575928,
      "learning_rate": 9.974672857745886e-06,
      "loss": 0.4952,
      "step": 171000
    },
    {
      "epoch": 1.5081961446461234,
      "grad_norm": 3.023170232772827,
      "learning_rate": 9.94535903569251e-06,
      "loss": 0.4966,
      "step": 171500
    },
    {
      "epoch": 1.5125932179541297,
      "grad_norm": 7.247273921966553,
      "learning_rate": 9.916045213639136e-06,
      "loss": 0.475,
      "step": 172000
    },
    {
      "epoch": 1.516990291262136,
      "grad_norm": 31.00918960571289,
      "learning_rate": 9.88673139158576e-06,
      "loss": 0.505,
      "step": 172500
    },
    {
      "epoch": 1.5213873645701421,
      "grad_norm": 6.270013332366943,
      "learning_rate": 9.857417569532387e-06,
      "loss": 0.4805,
      "step": 173000
    },
    {
      "epoch": 1.5257844378781482,
      "grad_norm": 12.974288940429688,
      "learning_rate": 9.828103747479012e-06,
      "loss": 0.4953,
      "step": 173500
    },
    {
      "epoch": 1.5301815111861545,
      "grad_norm": 4.286085605621338,
      "learning_rate": 9.798789925425638e-06,
      "loss": 0.4711,
      "step": 174000
    },
    {
      "epoch": 1.5345785844941608,
      "grad_norm": 17.97281265258789,
      "learning_rate": 9.769476103372263e-06,
      "loss": 0.494,
      "step": 174500
    },
    {
      "epoch": 1.5389756578021667,
      "grad_norm": 16.421478271484375,
      "learning_rate": 9.740162281318888e-06,
      "loss": 0.4991,
      "step": 175000
    },
    {
      "epoch": 1.543372731110173,
      "grad_norm": 3.0902822017669678,
      "learning_rate": 9.710848459265514e-06,
      "loss": 0.4924,
      "step": 175500
    },
    {
      "epoch": 1.5477698044181794,
      "grad_norm": 4.1982622146606445,
      "learning_rate": 9.681534637212139e-06,
      "loss": 0.4964,
      "step": 176000
    },
    {
      "epoch": 1.5521668777261854,
      "grad_norm": 4.393054962158203,
      "learning_rate": 9.652220815158766e-06,
      "loss": 0.4764,
      "step": 176500
    },
    {
      "epoch": 1.5565639510341915,
      "grad_norm": 5.67329740524292,
      "learning_rate": 9.62290699310539e-06,
      "loss": 0.4857,
      "step": 177000
    },
    {
      "epoch": 1.5609610243421979,
      "grad_norm": 8.207106590270996,
      "learning_rate": 9.593593171052015e-06,
      "loss": 0.4854,
      "step": 177500
    },
    {
      "epoch": 1.5653580976502042,
      "grad_norm": 5.108180999755859,
      "learning_rate": 9.56427934899864e-06,
      "loss": 0.4892,
      "step": 178000
    },
    {
      "epoch": 1.5697551709582103,
      "grad_norm": 4.476506233215332,
      "learning_rate": 9.534965526945265e-06,
      "loss": 0.4831,
      "step": 178500
    },
    {
      "epoch": 1.5741522442662164,
      "grad_norm": 6.414947032928467,
      "learning_rate": 9.505651704891891e-06,
      "loss": 0.483,
      "step": 179000
    },
    {
      "epoch": 1.5785493175742227,
      "grad_norm": 35.713966369628906,
      "learning_rate": 9.476337882838516e-06,
      "loss": 0.5056,
      "step": 179500
    },
    {
      "epoch": 1.5829463908822288,
      "grad_norm": 8.602806091308594,
      "learning_rate": 9.447024060785143e-06,
      "loss": 0.4877,
      "step": 180000
    },
    {
      "epoch": 1.5873434641902349,
      "grad_norm": 12.005407333374023,
      "learning_rate": 9.417710238731768e-06,
      "loss": 0.4869,
      "step": 180500
    },
    {
      "epoch": 1.5917405374982412,
      "grad_norm": 8.106757164001465,
      "learning_rate": 9.388396416678392e-06,
      "loss": 0.4869,
      "step": 181000
    },
    {
      "epoch": 1.5961376108062475,
      "grad_norm": 49.96762466430664,
      "learning_rate": 9.359082594625019e-06,
      "loss": 0.4967,
      "step": 181500
    },
    {
      "epoch": 1.6005346841142536,
      "grad_norm": 6.038918972015381,
      "learning_rate": 9.329768772571644e-06,
      "loss": 0.4942,
      "step": 182000
    },
    {
      "epoch": 1.6049317574222597,
      "grad_norm": 4.5768866539001465,
      "learning_rate": 9.30045495051827e-06,
      "loss": 0.4873,
      "step": 182500
    },
    {
      "epoch": 1.609328830730266,
      "grad_norm": 57.18939208984375,
      "learning_rate": 9.271141128464895e-06,
      "loss": 0.4843,
      "step": 183000
    },
    {
      "epoch": 1.613725904038272,
      "grad_norm": 3.1413328647613525,
      "learning_rate": 9.24182730641152e-06,
      "loss": 0.4815,
      "step": 183500
    },
    {
      "epoch": 1.6181229773462782,
      "grad_norm": 10.192014694213867,
      "learning_rate": 9.212513484358145e-06,
      "loss": 0.5001,
      "step": 184000
    },
    {
      "epoch": 1.6225200506542845,
      "grad_norm": 3.8693771362304688,
      "learning_rate": 9.18319966230477e-06,
      "loss": 0.4987,
      "step": 184500
    },
    {
      "epoch": 1.6269171239622908,
      "grad_norm": 14.650988578796387,
      "learning_rate": 9.153885840251396e-06,
      "loss": 0.4763,
      "step": 185000
    },
    {
      "epoch": 1.631314197270297,
      "grad_norm": 13.709172248840332,
      "learning_rate": 9.12457201819802e-06,
      "loss": 0.4917,
      "step": 185500
    },
    {
      "epoch": 1.635711270578303,
      "grad_norm": 4.213066577911377,
      "learning_rate": 9.095258196144647e-06,
      "loss": 0.5009,
      "step": 186000
    },
    {
      "epoch": 1.6401083438863093,
      "grad_norm": 9.890546798706055,
      "learning_rate": 9.065944374091272e-06,
      "loss": 0.4817,
      "step": 186500
    },
    {
      "epoch": 1.6445054171943154,
      "grad_norm": 6.438180923461914,
      "learning_rate": 9.036630552037897e-06,
      "loss": 0.4857,
      "step": 187000
    },
    {
      "epoch": 1.6489024905023215,
      "grad_norm": 6.375490665435791,
      "learning_rate": 9.007316729984523e-06,
      "loss": 0.4838,
      "step": 187500
    },
    {
      "epoch": 1.6532995638103278,
      "grad_norm": 8.293659210205078,
      "learning_rate": 8.978002907931148e-06,
      "loss": 0.4977,
      "step": 188000
    },
    {
      "epoch": 1.6576966371183341,
      "grad_norm": 8.562904357910156,
      "learning_rate": 8.948689085877775e-06,
      "loss": 0.4948,
      "step": 188500
    },
    {
      "epoch": 1.6620937104263402,
      "grad_norm": 8.690720558166504,
      "learning_rate": 8.9193752638244e-06,
      "loss": 0.4797,
      "step": 189000
    },
    {
      "epoch": 1.6664907837343463,
      "grad_norm": 8.591520309448242,
      "learning_rate": 8.890061441771024e-06,
      "loss": 0.4947,
      "step": 189500
    },
    {
      "epoch": 1.6708878570423527,
      "grad_norm": 5.863158702850342,
      "learning_rate": 8.86074761971765e-06,
      "loss": 0.4934,
      "step": 190000
    },
    {
      "epoch": 1.6752849303503587,
      "grad_norm": 7.344442367553711,
      "learning_rate": 8.831433797664276e-06,
      "loss": 0.4916,
      "step": 190500
    },
    {
      "epoch": 1.6796820036583648,
      "grad_norm": 10.189640045166016,
      "learning_rate": 8.8021199756109e-06,
      "loss": 0.4714,
      "step": 191000
    },
    {
      "epoch": 1.6840790769663712,
      "grad_norm": 7.846888542175293,
      "learning_rate": 8.772806153557525e-06,
      "loss": 0.4924,
      "step": 191500
    },
    {
      "epoch": 1.6884761502743775,
      "grad_norm": 8.43209457397461,
      "learning_rate": 8.743492331504152e-06,
      "loss": 0.491,
      "step": 192000
    },
    {
      "epoch": 1.6928732235823836,
      "grad_norm": 3.44476318359375,
      "learning_rate": 8.714178509450777e-06,
      "loss": 0.4936,
      "step": 192500
    },
    {
      "epoch": 1.6972702968903897,
      "grad_norm": 8.541385650634766,
      "learning_rate": 8.684864687397402e-06,
      "loss": 0.489,
      "step": 193000
    },
    {
      "epoch": 1.701667370198396,
      "grad_norm": 11.067632675170898,
      "learning_rate": 8.655550865344028e-06,
      "loss": 0.4863,
      "step": 193500
    },
    {
      "epoch": 1.7060644435064023,
      "grad_norm": 121.52491760253906,
      "learning_rate": 8.626237043290653e-06,
      "loss": 0.4948,
      "step": 194000
    },
    {
      "epoch": 1.7104615168144082,
      "grad_norm": 10.428354263305664,
      "learning_rate": 8.59692322123728e-06,
      "loss": 0.4997,
      "step": 194500
    },
    {
      "epoch": 1.7148585901224145,
      "grad_norm": 4.01568603515625,
      "learning_rate": 8.567609399183904e-06,
      "loss": 0.4961,
      "step": 195000
    },
    {
      "epoch": 1.7192556634304208,
      "grad_norm": 10.734025001525879,
      "learning_rate": 8.538295577130529e-06,
      "loss": 0.4778,
      "step": 195500
    },
    {
      "epoch": 1.723652736738427,
      "grad_norm": 7.982125282287598,
      "learning_rate": 8.508981755077156e-06,
      "loss": 0.4907,
      "step": 196000
    },
    {
      "epoch": 1.728049810046433,
      "grad_norm": 3.9649789333343506,
      "learning_rate": 8.47966793302378e-06,
      "loss": 0.4877,
      "step": 196500
    },
    {
      "epoch": 1.7324468833544393,
      "grad_norm": 24.0349178314209,
      "learning_rate": 8.450354110970405e-06,
      "loss": 0.4976,
      "step": 197000
    },
    {
      "epoch": 1.7368439566624456,
      "grad_norm": 4.649524211883545,
      "learning_rate": 8.42104028891703e-06,
      "loss": 0.496,
      "step": 197500
    },
    {
      "epoch": 1.7412410299704517,
      "grad_norm": 4.211353302001953,
      "learning_rate": 8.391726466863657e-06,
      "loss": 0.474,
      "step": 198000
    },
    {
      "epoch": 1.7456381032784578,
      "grad_norm": 8.621967315673828,
      "learning_rate": 8.362412644810281e-06,
      "loss": 0.484,
      "step": 198500
    },
    {
      "epoch": 1.7500351765864641,
      "grad_norm": 2.1576061248779297,
      "learning_rate": 8.333098822756906e-06,
      "loss": 0.4973,
      "step": 199000
    },
    {
      "epoch": 1.7544322498944702,
      "grad_norm": 3.1275100708007812,
      "learning_rate": 8.303785000703533e-06,
      "loss": 0.4818,
      "step": 199500
    },
    {
      "epoch": 1.7588293232024763,
      "grad_norm": 2.019500732421875,
      "learning_rate": 8.274471178650157e-06,
      "loss": 0.48,
      "step": 200000
    },
    {
      "epoch": 1.7632263965104826,
      "grad_norm": 7.910741806030273,
      "learning_rate": 8.245157356596784e-06,
      "loss": 0.4909,
      "step": 200500
    },
    {
      "epoch": 1.767623469818489,
      "grad_norm": 7.063455104827881,
      "learning_rate": 8.215843534543409e-06,
      "loss": 0.4805,
      "step": 201000
    },
    {
      "epoch": 1.772020543126495,
      "grad_norm": 6.943243980407715,
      "learning_rate": 8.186529712490034e-06,
      "loss": 0.4805,
      "step": 201500
    },
    {
      "epoch": 1.7764176164345011,
      "grad_norm": 3.7929699420928955,
      "learning_rate": 8.15721589043666e-06,
      "loss": 0.4809,
      "step": 202000
    },
    {
      "epoch": 1.7808146897425075,
      "grad_norm": 5.271979331970215,
      "learning_rate": 8.127902068383285e-06,
      "loss": 0.4784,
      "step": 202500
    },
    {
      "epoch": 1.7852117630505135,
      "grad_norm": 17.002561569213867,
      "learning_rate": 8.09858824632991e-06,
      "loss": 0.4823,
      "step": 203000
    },
    {
      "epoch": 1.7896088363585196,
      "grad_norm": 5.194180965423584,
      "learning_rate": 8.069274424276535e-06,
      "loss": 0.4897,
      "step": 203500
    },
    {
      "epoch": 1.794005909666526,
      "grad_norm": 5.924878120422363,
      "learning_rate": 8.039960602223161e-06,
      "loss": 0.4956,
      "step": 204000
    },
    {
      "epoch": 1.7984029829745323,
      "grad_norm": 8.067991256713867,
      "learning_rate": 8.010646780169786e-06,
      "loss": 0.4844,
      "step": 204500
    },
    {
      "epoch": 1.8028000562825384,
      "grad_norm": 12.200238227844238,
      "learning_rate": 7.98133295811641e-06,
      "loss": 0.4618,
      "step": 205000
    },
    {
      "epoch": 1.8071971295905445,
      "grad_norm": 4.0691633224487305,
      "learning_rate": 7.952019136063037e-06,
      "loss": 0.4709,
      "step": 205500
    },
    {
      "epoch": 1.8115942028985508,
      "grad_norm": 4.8085408210754395,
      "learning_rate": 7.922705314009662e-06,
      "loss": 0.4792,
      "step": 206000
    },
    {
      "epoch": 1.8159912762065569,
      "grad_norm": 3.7308754920959473,
      "learning_rate": 7.893391491956289e-06,
      "loss": 0.4882,
      "step": 206500
    },
    {
      "epoch": 1.820388349514563,
      "grad_norm": 7.143253326416016,
      "learning_rate": 7.864077669902913e-06,
      "loss": 0.4906,
      "step": 207000
    },
    {
      "epoch": 1.8247854228225693,
      "grad_norm": 12.130468368530273,
      "learning_rate": 7.83476384784954e-06,
      "loss": 0.5014,
      "step": 207500
    },
    {
      "epoch": 1.8291824961305756,
      "grad_norm": 8.273314476013184,
      "learning_rate": 7.805450025796165e-06,
      "loss": 0.4869,
      "step": 208000
    },
    {
      "epoch": 1.8335795694385817,
      "grad_norm": 9.056222915649414,
      "learning_rate": 7.77613620374279e-06,
      "loss": 0.4846,
      "step": 208500
    },
    {
      "epoch": 1.8379766427465878,
      "grad_norm": 8.364386558532715,
      "learning_rate": 7.746822381689414e-06,
      "loss": 0.4762,
      "step": 209000
    },
    {
      "epoch": 1.842373716054594,
      "grad_norm": 5.594216346740723,
      "learning_rate": 7.71750855963604e-06,
      "loss": 0.4821,
      "step": 209500
    },
    {
      "epoch": 1.8467707893626004,
      "grad_norm": 5.7328691482543945,
      "learning_rate": 7.688194737582666e-06,
      "loss": 0.4771,
      "step": 210000
    },
    {
      "epoch": 1.8511678626706063,
      "grad_norm": 4.169369697570801,
      "learning_rate": 7.65888091552929e-06,
      "loss": 0.4828,
      "step": 210500
    },
    {
      "epoch": 1.8555649359786126,
      "grad_norm": 4.151822090148926,
      "learning_rate": 7.629567093475915e-06,
      "loss": 0.4758,
      "step": 211000
    },
    {
      "epoch": 1.859962009286619,
      "grad_norm": 7.082273960113525,
      "learning_rate": 7.600253271422542e-06,
      "loss": 0.4793,
      "step": 211500
    },
    {
      "epoch": 1.864359082594625,
      "grad_norm": 8.685580253601074,
      "learning_rate": 7.570939449369167e-06,
      "loss": 0.465,
      "step": 212000
    },
    {
      "epoch": 1.868756155902631,
      "grad_norm": 12.204788208007812,
      "learning_rate": 7.541625627315793e-06,
      "loss": 0.4634,
      "step": 212500
    },
    {
      "epoch": 1.8731532292106374,
      "grad_norm": 9.337820053100586,
      "learning_rate": 7.512311805262418e-06,
      "loss": 0.4792,
      "step": 213000
    },
    {
      "epoch": 1.8775503025186437,
      "grad_norm": 6.66147518157959,
      "learning_rate": 7.482997983209044e-06,
      "loss": 0.4843,
      "step": 213500
    },
    {
      "epoch": 1.8819473758266496,
      "grad_norm": 3.0840163230895996,
      "learning_rate": 7.4536841611556685e-06,
      "loss": 0.4816,
      "step": 214000
    },
    {
      "epoch": 1.886344449134656,
      "grad_norm": 3.901893377304077,
      "learning_rate": 7.424370339102293e-06,
      "loss": 0.4891,
      "step": 214500
    },
    {
      "epoch": 1.8907415224426622,
      "grad_norm": 6.939720153808594,
      "learning_rate": 7.39505651704892e-06,
      "loss": 0.4744,
      "step": 215000
    },
    {
      "epoch": 1.8951385957506683,
      "grad_norm": 5.680385112762451,
      "learning_rate": 7.365742694995545e-06,
      "loss": 0.4795,
      "step": 215500
    },
    {
      "epoch": 1.8995356690586744,
      "grad_norm": 21.55387306213379,
      "learning_rate": 7.33642887294217e-06,
      "loss": 0.481,
      "step": 216000
    },
    {
      "epoch": 1.9039327423666808,
      "grad_norm": 4.046156883239746,
      "learning_rate": 7.307115050888796e-06,
      "loss": 0.4969,
      "step": 216500
    },
    {
      "epoch": 1.908329815674687,
      "grad_norm": 2.8407113552093506,
      "learning_rate": 7.277801228835422e-06,
      "loss": 0.4804,
      "step": 217000
    },
    {
      "epoch": 1.9127268889826932,
      "grad_norm": 11.015763282775879,
      "learning_rate": 7.2484874067820464e-06,
      "loss": 0.483,
      "step": 217500
    },
    {
      "epoch": 1.9171239622906993,
      "grad_norm": 7.1212592124938965,
      "learning_rate": 7.219173584728671e-06,
      "loss": 0.4821,
      "step": 218000
    },
    {
      "epoch": 1.9215210355987056,
      "grad_norm": 3.4284260272979736,
      "learning_rate": 7.189859762675298e-06,
      "loss": 0.4782,
      "step": 218500
    },
    {
      "epoch": 1.9259181089067117,
      "grad_norm": 12.792159080505371,
      "learning_rate": 7.160545940621923e-06,
      "loss": 0.4743,
      "step": 219000
    },
    {
      "epoch": 1.9303151822147178,
      "grad_norm": 8.712377548217773,
      "learning_rate": 7.131232118568548e-06,
      "loss": 0.4561,
      "step": 219500
    },
    {
      "epoch": 1.934712255522724,
      "grad_norm": 2.8319313526153564,
      "learning_rate": 7.101918296515173e-06,
      "loss": 0.4776,
      "step": 220000
    },
    {
      "epoch": 1.9391093288307304,
      "grad_norm": 3.13771653175354,
      "learning_rate": 7.072604474461799e-06,
      "loss": 0.4655,
      "step": 220500
    },
    {
      "epoch": 1.9435064021387365,
      "grad_norm": 6.257668495178223,
      "learning_rate": 7.043290652408424e-06,
      "loss": 0.484,
      "step": 221000
    },
    {
      "epoch": 1.9479034754467426,
      "grad_norm": 5.85142183303833,
      "learning_rate": 7.013976830355049e-06,
      "loss": 0.4802,
      "step": 221500
    },
    {
      "epoch": 1.952300548754749,
      "grad_norm": 7.446153163909912,
      "learning_rate": 6.984663008301676e-06,
      "loss": 0.4632,
      "step": 222000
    },
    {
      "epoch": 1.956697622062755,
      "grad_norm": 14.04566764831543,
      "learning_rate": 6.9553491862483006e-06,
      "loss": 0.4731,
      "step": 222500
    },
    {
      "epoch": 1.961094695370761,
      "grad_norm": 4.769692897796631,
      "learning_rate": 6.926035364194926e-06,
      "loss": 0.4735,
      "step": 223000
    },
    {
      "epoch": 1.9654917686787674,
      "grad_norm": 3.6728134155273438,
      "learning_rate": 6.896721542141551e-06,
      "loss": 0.4797,
      "step": 223500
    },
    {
      "epoch": 1.9698888419867737,
      "grad_norm": 7.23699951171875,
      "learning_rate": 6.867407720088176e-06,
      "loss": 0.4824,
      "step": 224000
    },
    {
      "epoch": 1.9742859152947798,
      "grad_norm": 3.4703140258789062,
      "learning_rate": 6.838093898034802e-06,
      "loss": 0.4869,
      "step": 224500
    },
    {
      "epoch": 1.978682988602786,
      "grad_norm": 5.19166898727417,
      "learning_rate": 6.808780075981427e-06,
      "loss": 0.4732,
      "step": 225000
    },
    {
      "epoch": 1.9830800619107922,
      "grad_norm": 6.126416206359863,
      "learning_rate": 6.779466253928053e-06,
      "loss": 0.4751,
      "step": 225500
    },
    {
      "epoch": 1.9874771352187983,
      "grad_norm": 7.575957298278809,
      "learning_rate": 6.750152431874678e-06,
      "loss": 0.4732,
      "step": 226000
    },
    {
      "epoch": 1.9918742085268044,
      "grad_norm": 8.700682640075684,
      "learning_rate": 6.720838609821303e-06,
      "loss": 0.4915,
      "step": 226500
    },
    {
      "epoch": 1.9962712818348107,
      "grad_norm": 14.277158737182617,
      "learning_rate": 6.691524787767929e-06,
      "loss": 0.4608,
      "step": 227000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7725321039197451,
      "eval_f1": 0.7724015046525441,
      "eval_loss": 0.4883246123790741,
      "eval_precision": 0.7728146108591011,
      "eval_recall": 0.7719888398599046,
      "eval_runtime": 43.6116,
      "eval_samples_per_second": 2317.684,
      "eval_steps_per_second": 144.87,
      "step": 227424
    }
  ],
  "logging_steps": 500,
  "max_steps": 341136,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.91769913176308e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
